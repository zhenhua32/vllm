{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证下在贪婪解码的情况下, transformers 和 vllm 的推理结果是否一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本设置\n",
    "import torch\n",
    "\n",
    "# model_dir = \"/mnt/g/code/pretrain_model_dir/_modelscope/qwen/Qwen2-7B-Instruct\"  # 加载需要 7分半\n",
    "model_dir = \"/root/home/my_model/Qwen2-7B-Instruct\"  # 爷受伤了, wsl2 的 io 也太差了\n",
    "\n",
    "torch_dtype_str = \"bfloat16\"\n",
    "torch_dtype = torch.bfloat16\n",
    "# torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a1ec5362204af29e536ff1fdb697ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16 cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "print(model.dtype, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！很高兴能为你提供帮助。有什么问题或需要我解答的吗？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor([108386,   6313, 112169,  26232, 106184,  99553, 100364,   1773, 104139,\n",
       "           86119,  57191,  85106,  35946, 106185,   9370, 101037,  11319, 151645],\n",
       "         device='cuda:0')],\n",
       " '你好！很高兴能为你提供帮助。有什么问题或需要我解答的吗？')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_greddy_chat(messages: list[str]):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(response)\n",
    "\n",
    "    return generated_ids, response\n",
    "\n",
    "\n",
    "prompt = \"你好\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "generate_greddy_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greddy_text(text: str):\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    print(model_inputs.input_ids,)\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(response)\n",
    "\n",
    "    return generated_ids, response\n",
    "\n",
    "# generate_greddy_text(\"你好,世界\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[108386,     11,  99489]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/home/anaconda3/envs/vllm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/home/anaconda3/envs/vllm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/root/home/anaconda3/envs/vllm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:537: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!——记2017级新生开学典礼\n",
      "\n",
      "金秋九月,丹桂飘香。在这收获的季节里,我们迎来了\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor([     0,   8545,  40814,     17,     15,     16,     22,  52334, 102900,\n",
       "          107823, 113953,    271,  34230, 100057,  99609,   9754,     11, 100721,\n",
       "          100877, 104712,  99662,   1773, 102332, 104619,   9370, 105419,  69249,\n",
       "              11,  97639, 108646], device='cuda:0')],\n",
       " '!——记2017级新生开学典礼\\n\\n金秋九月,丹桂飘香。在这收获的季节里,我们迎来了')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n\"\n",
    "example = \"你好,世界\"\n",
    "generate_greddy_text(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
