{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本设置\n",
    "\n",
    "# model_dir = \"/mnt/g/code/pretrain_model_dir/_modelscope/qwen/Qwen2-7B-Instruct\"  # 加载需要 7分半\n",
    "model_dir = \"/root/home/my_model/Qwen2-7B-Instruct\"  # 爷受伤了, wsl2 的 io 也太差了\n",
    "\n",
    "torch_dtype_str = \"bfloat16\"\n",
    "# torch_dtype_str = \"float16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-22 16:36:35 llm_engine.py:164] Initializing an LLM engine (v0.5.0.post1) with config: model='/root/home/my_model/Qwen2-7B-Instruct', speculative_config=None, tokenizer='/root/home/my_model/Qwen2-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/root/home/my_model/Qwen2-7B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-22 16:36:36 utils.py:494] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 06-22 16:36:41 model_runner.py:164] Loading model weights took 14.2487 GB\n",
      "INFO 06-22 16:36:42 gpu_executor.py:83] # GPU blocks: 6941, # CPU blocks: 4681\n",
      "INFO 06-22 16:36:42 model_runner.py:901] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-22 16:36:42 model_runner.py:905] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-22 16:36:51 model_runner.py:977] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model_dir, dtype=torch_dtype_str, max_model_len=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 4.35 toks/s, output: 43.53 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=30)\n",
    "\n",
    "\n",
    "# example = \"vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n\"\n",
    "example = \"你好,世界\"\n",
    "outputs = llm.generate([example], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好,世界'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs[0])\n",
    "tokenizer.decode(outputs[0].prompt_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestOutput(request_id=1, prompt='你好,世界', prompt_token_ids=[108386, 11, 99489], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='!——记2021级新生开学典礼\\n\\n2021年9月1日,在秋风送爽、丹桂', token_ids=[0, 8545, 40814, 17, 15, 17, 16, 52334, 102900, 107823, 113953, 271, 17, 15, 17, 16, 7948, 24, 9754, 16, 8903, 11, 18493, 100057, 99208, 36605, 102308, 5373, 100721, 100877], cumulative_logprob=-28.88200148125179, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1719046487.7759755, last_token_time=1719046487.7759755, first_scheduled_time=1719046487.7781467, first_token_time=1719046487.8039474, time_in_queue=0.0021712779998779297, finished_time=1719046488.467085), lora_request=None)\n",
      "--\n",
      "[CompletionOutput(index=0, text='!——记2021级新生开学典礼\\n\\n2021年9月1日,在秋风送爽、丹桂', token_ids=[0, 8545, 40814, 17, 15, 17, 16, 52334, 102900, 107823, 113953, 271, 17, 15, 17, 16, 7948, 24, 9754, 16, 8903, 11, 18493, 100057, 99208, 36605, 102308, 5373, 100721, 100877], cumulative_logprob=-28.88200148125179, logprobs=None, finish_reason=length, stop_reason=None)]\n",
      "--\n",
      "!——记2021级新生开学典礼\n",
      "\n",
      "2021年9月1日,在秋风送爽、丹桂\n",
      "[0, 8545, 40814, 17, 15, 17, 16, 52334, 102900, 107823, 113953, 271, 17, 15, 17, 16, 7948, 24, 9754, 16, 8903, 11, 18493, 100057, 99208, 36605, 102308, 5373, 100721, 100877]\n",
      "!——记2021级新生开学典礼\n",
      "\n",
      "2021年9月1日,在秋风送爽、丹桂\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    print(output)\n",
    "    print(\"--\")\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(output.outputs)\n",
    "    print(\"--\")\n",
    "\n",
    "    print(generated_text)\n",
    "    print(output.outputs[0].token_ids)\n",
    "    print(tokenizer.decode(output.outputs[0].token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
